{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 2)\n",
      "40000\n",
      "                                               Tweet  Class\n",
      "0  \"Awesome case, plus those blue LEDs make your ...      1\n",
      "1  \"I don't know man. [This](http://www.reddit.co...      1\n",
      "2           \"because he is famous  Edit: oh yeah /s\"      1\n",
      "3  \"&gt; My deputies did their job to the fullest...      1\n",
      "4  \"Because what better way to woo affection from...      1\n",
      "                                                   Tweet  Class\n",
      "19995                                              \"yep\"      0\n",
      "19996      \"OK who's available now or within next hour?\"      0\n",
      "19997                                        \"[deleted]\"      0\n",
      "19998  \"I hate it when I go to hug someone really sex...      1\n",
      "19999         \"Not all men are annoying. Some are dead.\"      1\n",
      "Index(['Tweet', 'Class'], dtype='object')\n",
      "              Class\n",
      "count  20000.000000\n",
      "mean       0.500050\n",
      "std        0.500012\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        1.000000\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 2 columns):\n",
      "Tweet    20000 non-null object\n",
      "Class    20000 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 312.6+ KB\n",
      "None\n",
      "False\n",
      "       Tweet  Class\n",
      "0      False  False\n",
      "1      False  False\n",
      "2      False  False\n",
      "3      False  False\n",
      "4      False  False\n",
      "...      ...    ...\n",
      "19995  False  False\n",
      "19996  False  False\n",
      "19997  False  False\n",
      "19998  False  False\n",
      "19999  False  False\n",
      "\n",
      "[20000 rows x 2 columns]\n",
      "Tweet    False\n",
      "Class    False\n",
      "dtype: bool\n",
      "Tweet    0\n",
      "Class    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\audit\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\audit\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\audit\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________\n",
      "*****  result  ******\n",
      " not scaristic\n",
      "_______________________________________\n"
     ]
    }
   ],
   "source": [
    "#importing the necessary libary and packages\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "#reading the preprocessed data\n",
    "data=pd.read_csv(\"Sarcastic1.csv\")\n",
    "\n",
    "#performing EDA of the preprocessed data\n",
    "print(data.shape)\n",
    "print(data.size)\n",
    "print(data.head())\n",
    "print(data.tail())\n",
    "print(data.columns)\n",
    "print(data.describe())\n",
    "print(data.info())\n",
    "print(data.isnull().values.any())\n",
    "print(data.isnull())\n",
    "print(data.isnull().any())\n",
    "print(data.isnull().sum())\n",
    "\n",
    "#feature selecting \n",
    "#selecting the input labels and outputlabels\n",
    "df_data = data[[\"Tweet\",\"Class\"]]\n",
    "\n",
    "# Features and Labels\n",
    "df_x = df_data['Tweet']\n",
    "df_y = df_data.Class\n",
    "\n",
    "\n",
    "#Extract Feature With CountVectorizer\n",
    "corpus = df_x\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus) # Fit the Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df_y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "#Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train,y_train)\n",
    "a1=clf.score(X_test,y_test)\n",
    "\n",
    "\n",
    "#logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lg=LogisticRegression()\n",
    "lg.fit(X_train,y_train)\n",
    "a2=lg.score(X_test,y_test)\n",
    "\n",
    "#decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dc=DecisionTreeClassifier()\n",
    "dc.fit(X_train,y_train)\n",
    "a3=dc.score(X_test,y_test)\n",
    "\n",
    "\n",
    "#randomforest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "a4=rf.score(X_test,y_test)\n",
    "\n",
    "\n",
    "#support vector machine\n",
    "from sklearn.svm import SVC\n",
    "sv=SVC()\n",
    "sv.fit(X_train,y_train)\n",
    "a5=sv.score(X_test,y_test)\n",
    "\n",
    "#input\n",
    "a=\"hi good morning\"\n",
    "inp=[a]\n",
    "vect = cv.transform(inp).toarray()\n",
    "my_prediction = clf.predict(vect)\n",
    "\n",
    "#craeting the pickle file for sacarsam model\n",
    "with open (\"scar.pickle\",\"wb\") as f:\n",
    "    pickle.dump(clf,f)\n",
    "    \n",
    "if my_prediction[0]==0:\n",
    "    print(\"_______________________________________\")\n",
    "    print(\"*****  result  ******\")\n",
    "    \n",
    "    print(\" not scaristic\")\n",
    "    print(\"_______________________________________\")\n",
    "elif my_prediction[0]==1:\n",
    "    print(\"_______________________________________\")\n",
    "    print(\"*****  result  ******\")\n",
    "    \n",
    "    print(\" scarsm\")\n",
    "    print(\"_______________________________________\")\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#craeting the pickle file for sacarsam model\n",
    "with open (\"scar.pickle\",\"wb\") as f:\n",
    "    pickle.dump(clf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
